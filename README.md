# GradCAM Implementation Comparison

This notebook compares two implementations of GradCAM: the `pytorch-grad-cam` library and the `explainable-cnn` library. The goal is to demonstrate the potential differences in the generated heatmaps and highlight a specific issue observed with the `explainable-cnn` implementation regarding color inversion.

## Overview

The notebook performs the following steps:

1.  **Data Download**: Downloads necessary image files and ImageNet class labels.
2.  **Library Installation**: Installs the required libraries, including `pytorch-grad-cam` and `explainable-cnn`.
3.  **Code Analysis and Modification**: (Includes steps taken to address compatibility issues with `explainable-cnn`).
4.  **GradCAM Generation**: Applies both GradCAM implementations to sample images using a pre-trained ResNet model and a custom-trained model (if the checkpoint is available).
5.  **Visualization**: Displays the original images alongside the GradCAM heatmaps generated by both methods for visual comparison.

## Requirements

*   Google Colab environment (or a Python environment with necessary libraries)
*   Access to the internet for downloading data and libraries.

## Usage

1.  **Run all cells**: Execute all the code cells in the notebook sequentially.
2.  **Observe the output**: The notebook will display comparison plots showing the original images and the GradCAM heatmaps from both `pytorch-grad-cam` and `explainable-cnn`.

## Files

*   `A12A_x2700_007.png`, `E14F.png`, `F13G.png`: Random spore images.
*   `dog.jpg`, `tiger_shark.jpeg`: Example ImageNet images.
*   `imagenet_class_labels.pkl`: ImageNet class labels.
*   `spores_model_cv_1_37.pth`: Random checkpoint for a trained model.

## Notes

*   The `explainable-cnn` library may require specific versions of dependencies, and the notebook includes steps taken to address a known compatibility issue with newer NumPy versions.
*   If the custom model checkpoint (`spores_model_cv_1_37.pth`) is not available, the notebook will still run the ImageNet examples.
*   The comparison highlights a potential color inversion issue with the `explainable-cnn` generated heatmaps compared to the standard implementation.

## Further Exploration

*   Experiment with different images.
*   Modify the code to use different pre-trained models.
*   Investigate the source code of `explainable-cnn` to understand the cause of the observed color inversion.
*   Apply the corrected heatmap (as shown in the original notebook's `show_corrected_version` function) to the `explainable-cnn` output for a direct visual comparison with the standard implementation.
